# NVIDIA Model Deployment Project

This project shows how to deploy models using Triton Inference Server.
It includes exporting a TensorFlow model, writing config.pbtxt, and sending inference requests.

## What I Did
- Used NVIDIA Triton Inference Server
- Deployed TensorFlow model
- Created config.pbtxt file
- Sent test requests using Python and curl

## Tools Used
- Triton Inference Server
- TensorFlow
- Python