\# NVIDIA Model Deployment Project



ğŸš€ \*\*Project Overview\*\*  

This project showcases the deployment of a PyTorch NLP model using TorchScript and Triton Inference Server, as part of NVIDIA DLI training.



âš™ï¸ \*\*Tech Stack\*\*

\- PyTorch, TorchScript

\- HuggingFace Transformers (XLM-Roberta)

\- Triton Inference Server

\- Docker, Linux CLI



ğŸ“¦ \*\*Features\*\*

\- Exported HuggingFace model using TorchScript

\- Created model repository with `config.pbtxt`

\- Deployed model on Triton Inference Server

\- Sent inference requests and received predictions



ğŸ“ \*\*Folder Structure\*\*

models/

â””â”€â”€ huggingface-model/

â”œâ”€â”€ 1/

â”‚ â””â”€â”€ model.pt

â””â”€â”€ config.pbtxt



ğŸ§ª \*\*How to Run\*\*

```bash

\# Export the model

python export\_model.py



\# Start Triton server

tritonserver --model-repository=models/



\# Send inference request

python client\_infer.py





ğŸ“ What I Learned



How to trace PyTorch models using TorchScript



How to configure and run Triton Server



Hands-on practice with model deployment and inference APIs

